{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.distribute import distribution_strategy_context as ds_context\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.keras import activations\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras import constraints\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.engine.base_layer import Layer\n",
    "from tensorflow.python.keras.engine.input_spec import InputSpec\n",
    "from tensorflow.python.keras.utils import generic_utils\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import control_flow_util\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.training.tracking import base as trackable\n",
    "from tensorflow.python.training.tracking import data_structures\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "from tensorflow.tools.docs import doc_controls\n",
    "\n",
    "class SLSTMCell(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    继承Layer并重载了5个函数，参照TF中LSTMCell源码并做了修改\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                   units,\n",
    "                   activation='tanh',\n",
    "                   recurrent_activation='hard_sigmoid',\n",
    "                   use_bias=True,\n",
    "                   kernel_initializer='glorot_uniform',\n",
    "                   recurrent_initializer='orthogonal',\n",
    "                   bias_initializer='zeros',\n",
    "                   unit_forget_bias=True,\n",
    "                   kernel_regularizer=None,\n",
    "                   recurrent_regularizer=None,\n",
    "                   bias_regularizer=None,\n",
    "                   kernel_constraint=None,\n",
    "                   recurrent_constraint=None,\n",
    "                   bias_constraint=None,\n",
    "                   dropout=0.,\n",
    "                   recurrent_dropout=0.,\n",
    "                   implementation=1,\n",
    "                   **kwargs):\n",
    "        #print(\"init start\")\n",
    "        self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n",
    "        super(SLSTMCell, self).__init__(**kwargs)\n",
    "        self.num_gates = 6\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.unit_forget_bias = unit_forget_bias\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        self.dropout = min(1., max(0., dropout))\n",
    "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "        if self.recurrent_dropout != 0 and implementation != 1:\n",
    "            logging.debug(RECURRENT_DROPOUT_WARNING_MSG)\n",
    "            self.implementation = 1\n",
    "        else:\n",
    "            self.implementation = implementation\n",
    "        # tuple(_ListWrapper) was silently dropping list content in at least 2.7.10,\n",
    "        # and fixed after 2.7.16. Converting the state_size to wrapper around\n",
    "        # NoDependency(), so that the base_layer.__setattr__ will not convert it to\n",
    "        # ListWrapper. Down the stream, self.states will be a list since it is\n",
    "        # generated from nest.map_structure with list, and tuple(list) will work\n",
    "        # properly.\n",
    "        self.state_size = data_structures.NoDependency([self.units, self.units])\n",
    "        self.output_size = self.units\n",
    "        \n",
    "        self.SeasonalGate = False\n",
    "        \n",
    "        #print(\"init end\")\n",
    "\n",
    "    \n",
    "    \n",
    "    @tf_utils.shape_type_conversion\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print(\"build start\")\n",
    "        input_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\n",
    "            # W_x: 用于和当前时刻隐层输入x做乘法的矩阵\n",
    "            # 加入季节门，将self.units * 4改为self.units * 6 \n",
    "            # 这里\n",
    "            shape=(input_dim, self.units * self.num_gates),\n",
    "            name='kernel',\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint)\n",
    "\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            # W_h: 用于和前一时刻隐层输出h做乘法的矩阵\n",
    "            # 加入季节门，将self.units * 4改为self.units * 6\n",
    "            shape=(self.units, self.units * self.num_gates),\n",
    "            name='recurrent_kernel',\n",
    "            initializer=self.recurrent_initializer,\n",
    "            regularizer=self.recurrent_regularizer,\n",
    "            constraint=self.recurrent_constraint)\n",
    "\n",
    "\n",
    "        if self.use_bias:\n",
    "            if self.unit_forget_bias:\n",
    "\n",
    "                def bias_initializer(_, *args, **kwargs):\n",
    "                    return K.concatenate([\n",
    "                          self.bias_initializer((self.units,), *args, **kwargs),\n",
    "                          initializers.Ones()((self.units,), *args, **kwargs),\n",
    "                          self.bias_initializer((self.units * (self.num_gates - 2),), *args, **kwargs),\n",
    "                      ])\n",
    "            else:\n",
    "                bias_initializer = self.bias_initializer\n",
    "            \n",
    "            self.bias = self.add_weight(\n",
    "                  shape=(self.units * self.num_gates,),\n",
    "                  name='bias',\n",
    "                  initializer=bias_initializer,\n",
    "                  regularizer=self.bias_regularizer,\n",
    "                  constraint=self.bias_constraint)\n",
    "\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.built = True\n",
    "        print(\"build end\")\n",
    "\n",
    "\n",
    "    def _compute_carry_and_output(self, x, h_tm1, c_tm1):\n",
    "        \"\"\"Computes carry and output using split kernels.\"\"\"\n",
    "        '''\n",
    "        方案一（注意call中还有）\n",
    "        通过短期信息对输入值进行分流，包括Wx+b向量的极差、标准化X、标准化X的极差\n",
    "        \n",
    "        模式1\n",
    "        采用分离法计算各个门控单元的值\n",
    "        本模式尝试将选通器的判断方法调整为h_tm1和x\n",
    "        本方法中x,h_tm1,c_tm1均为各个门加mask后拼成的元组，故需要多传多\n",
    "        模式1中的x是 Wx+b \n",
    "        '''\n",
    "        x_i, x_f, x_c, x_o, x_s1, x_s2 = x\n",
    "        h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o, h_tm1_s1, h_tm1_s2 = h_tm1\n",
    "        i = self.recurrent_activation(\n",
    "            x_i + K.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]))\n",
    "        f = self.recurrent_activation(x_f + K.dot(\n",
    "            h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))\n",
    "        \n",
    "        # 季节门\n",
    "        # 批差分\n",
    "        sdB2 = 1.82\n",
    "        sd = 2.41\n",
    "        sdT2 = 3.58\n",
    "\n",
    "        # 季节门1【改】\n",
    "        s1 = K.tanh(x_s1 + K.dot(\n",
    "            h_tm1_s1, self.recurrent_kernel[:, self.units * 4:self.units * 5]))\n",
    "        # 季节门2【改】\n",
    "        s2 = self.recurrent_activation(x_s2 + K.dot(\n",
    "            h_tm1_s2, self.recurrent_kernel[:, self.units * 5:]))\n",
    "        # 方案1*\n",
    "        #SGate = K.max(x_s1) - K.min(x_s1) > \n",
    "        #if SGate:\n",
    "        \n",
    "        # 方案1--还有call中的一部分\n",
    "        if self.SeasonalGate:\n",
    "            \n",
    "            c = f * c_tm1 * s1 + i * self.activation(x_c + K.dot(\n",
    "                h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))\n",
    "            o = self.recurrent_activation(\n",
    "                x_o + K.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:self.units * 4]))\n",
    "        else:\n",
    "            c = f * c_tm1 * s2 + i * self.activation(x_c + K.dot(\n",
    "                h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))\n",
    "            o = self.recurrent_activation(\n",
    "                x_o + K.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:self.units * 4]))\n",
    "\n",
    "        return c, o\n",
    "\n",
    "    def _compute_carry_and_output_fused(self, z, c_tm1):\n",
    "        \"\"\"\n",
    "        方案二\n",
    "        通过长期信息即细胞状态来分流\n",
    "        \n",
    "        模式2\n",
    "        Computes carry and output using fused kernels.\n",
    "        使用合并向量的计算方法\n",
    "        \"\"\"\n",
    "        \n",
    "        z0, z1, z2, z3, z4, z5 = z\n",
    "        i = self.recurrent_activation(z0)\n",
    "        f = self.recurrent_activation(z1)\n",
    "        \n",
    "        #季节门\n",
    "        sdB4 = 0.2828\n",
    "        sdB2 = 0.5102\n",
    "        sd = 0.7975\n",
    "        sdT2 = 0.9501\n",
    "        #if True:\n",
    "        if K.max(K.softmax(c_tm1)) - K.min(K.softmax(c_tm1)) > 0.7975: #【改】\n",
    "            #季节门1【改】\n",
    "            s = self.recurrent_activation(z4)\n",
    "            #s = K.square(s)\n",
    "        else:\n",
    "            #季节门2【改】\n",
    "            s = self.recurrent_activation(z5)\n",
    "            \n",
    "        #c = f * c_tm1 + i * self.activation(z2)\n",
    "        c = f * c_tm1 * s + i * self.activation(z2)\n",
    "        o = self.recurrent_activation(z3)\n",
    "        return c, o\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        num_gates = 6\n",
    "        h_tm1 = states[0]  # previous memory state\n",
    "        c_tm1 = states[1]  # previous carry state\n",
    "\n",
    "        #dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=num_gates)\n",
    "        \n",
    "        #rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n",
    "        #    h_tm1, training, count=num_gates)\n",
    "\n",
    "        if self.implementation == 1:\n",
    "            '''\n",
    "            implementation是计算模式，1或者2\n",
    "            \n",
    "            方案一（注意computer中还有）\n",
    "            即inputs值大于k倍标准差，根据实际计算\n",
    "            '''\n",
    "            if  K.mean(inputs) > 1 : \n",
    "                self.SeasonalGate = True\n",
    "\n",
    "            if 0 < self.dropout < 1.:\n",
    "                inputs_i = inputs * dp_mask[0]\n",
    "                inputs_f = inputs * dp_mask[1]\n",
    "                inputs_c = inputs * dp_mask[2]\n",
    "                inputs_o = inputs * dp_mask[3]\n",
    "                inputs_s1 = inputs * dp_mask[4]\n",
    "                inputs_s2 = inputs * dp_mask[5]\n",
    "            else:\n",
    "                inputs_i = inputs\n",
    "                inputs_f = inputs\n",
    "                inputs_c = inputs\n",
    "                inputs_o = inputs\n",
    "                inputs_s1 = inputs\n",
    "                inputs_s2 = inputs\n",
    "            k_i, k_f, k_c, k_o, k_s1, k_s2 = array_ops.split(\n",
    "                  self.kernel, num_or_size_splits=6, axis=1)\n",
    "            x_i = K.dot(inputs_i, k_i)\n",
    "            x_f = K.dot(inputs_f, k_f)\n",
    "            x_c = K.dot(inputs_c, k_c)\n",
    "            x_o = K.dot(inputs_o, k_o)\n",
    "            x_s1 = K.dot(inputs_o, k_s1)\n",
    "            x_s2 = K.dot(inputs_o, k_s2)\n",
    "            if self.use_bias:\n",
    "                b_i, b_f, b_c, b_o, b_s1, b_s2 = array_ops.split(\n",
    "                    self.bias, num_or_size_splits=6, axis=0)\n",
    "                x_i = K.bias_add(x_i, b_i)\n",
    "                x_f = K.bias_add(x_f, b_f)\n",
    "                x_c = K.bias_add(x_c, b_c)\n",
    "                x_o = K.bias_add(x_o, b_o)\n",
    "                x_s1 = K.bias_add(x_o, b_s1)\n",
    "                x_s2 = K.bias_add(x_o, b_s2)\n",
    "\n",
    "            if 0 < self.recurrent_dropout < 1.:\n",
    "                h_tm1_i = h_tm1 * rec_dp_mask[0]\n",
    "                h_tm1_f = h_tm1 * rec_dp_mask[1]\n",
    "                h_tm1_c = h_tm1 * rec_dp_mask[2]\n",
    "                h_tm1_o = h_tm1 * rec_dp_mask[3]\n",
    "                h_tm1_s1 = h_tm1 * rec_dp_mask[4]\n",
    "                h_tm1_s2 = h_tm1 * rec_dp_mask[5]\n",
    "            else:\n",
    "                h_tm1_i = h_tm1\n",
    "                h_tm1_f = h_tm1\n",
    "                h_tm1_c = h_tm1\n",
    "                h_tm1_o = h_tm1\n",
    "                h_tm1_s1 = h_tm1\n",
    "                h_tm1_s2 = h_tm1\n",
    "            x = (x_i, x_f, x_c, x_o, x_s1, x_s2)\n",
    "            h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o,h_tm1_s1,h_tm1_s2)\n",
    "            c, o = self._compute_carry_and_output(x, h_tm1, c_tm1)\n",
    "        else:\n",
    "            # 模式2\n",
    "            # 以下为使用随机遮挡的方法\n",
    "            if 0. < self.dropout < 1.:\n",
    "                inputs = inputs * dp_mask[0]\n",
    "            \n",
    "            z = K.dot(inputs, self.kernel)\n",
    "            #不是z = K.dot(self.kernel, inputs)，因为x_t是行向量\n",
    "            z += K.dot(h_tm1,self.recurrent_kernel)\n",
    "            #不是z += K.dot(self.recurrent_kernel, h_tm1)，因为x_t是行向量\n",
    "            if self.use_bias:\n",
    "                z = K.bias_add(z, self.bias)\n",
    "            \n",
    "            z = array_ops.split(z, num_or_size_splits=num_gates, axis=1)\n",
    "            c, o = self._compute_carry_and_output_fused(z, c_tm1)\n",
    "\n",
    "        h = o * self.activation(c)\n",
    "        return h, [h, c]\n",
    "    \n",
    "    def get_config(self):\n",
    "        print(\"config start\")\n",
    "\n",
    "        config = {\n",
    "            'units':\n",
    "                self.units,\n",
    "            'activation':\n",
    "                activations.serialize(self.activation),\n",
    "            'recurrent_activation':\n",
    "                activations.serialize(self.recurrent_activation),\n",
    "            'use_bias':\n",
    "                self.use_bias,\n",
    "            'kernel_initializer':\n",
    "                initializers.serialize(self.kernel_initializer),\n",
    "            'recurrent_initializer':\n",
    "                initializers.serialize(self.recurrent_initializer),\n",
    "            'bias_initializer':\n",
    "                initializers.serialize(self.bias_initializer),\n",
    "            'unit_forget_bias':\n",
    "                self.unit_forget_bias,\n",
    "            'kernel_regularizer':\n",
    "                regularizers.serialize(self.kernel_regularizer),\n",
    "            'recurrent_regularizer':\n",
    "                regularizers.serialize(self.recurrent_regularizer),\n",
    "            'bias_regularizer':\n",
    "                regularizers.serialize(self.bias_regularizer),\n",
    "            'kernel_constraint':\n",
    "                constraints.serialize(self.kernel_constraint),\n",
    "            'recurrent_constraint':\n",
    "                constraints.serialize(self.recurrent_constraint),\n",
    "            'bias_constraint':\n",
    "                constraints.serialize(self.bias_constraint),\n",
    "            'dropout':\n",
    "                self.dropout,\n",
    "            'recurrent_dropout':\n",
    "                self.recurrent_dropout,\n",
    "            'implementation':\n",
    "                self.implementation\n",
    "        }\n",
    "        base_config = super(SLSTMCell, self).get_config()\n",
    "        print(\"config end\")\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "        \n",
    "\n",
    "    '''\n",
    "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "        return list(_generate_zero_filled_state_for_cell(\n",
    "            self, inputs, batch_size, dtype))\n",
    "    \n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 航空数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AirPassengers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Date  AirPassengers\n",
       "0     1            112\n",
       "1     2            118\n",
       "2     3            132\n",
       "3     4            129\n",
       "4     5            121"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "filename = \"AirPassengers.csv\"\n",
    "f=open(filename,encoding='UTF-8')\n",
    "#names=['Date','Num']\n",
    "df = pd.read_csv(f,header=0)\n",
    "df.columns = ['Date','AirPassengers']\n",
    "#df = df.set_index(, drop = True)\n",
    "#df = df.set_index(\"Date\")\n",
    "#print(data[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def univariate_data(dataset, \n",
    "                    start_index, end_index, \n",
    "                    history_size, target_size, \n",
    "                    single_step=True):\n",
    "    '''\n",
    "    dataset: 自变量\n",
    "    start_index: 训练集或测试集的第一条所在行\n",
    "    end_index: 训练集或测试集的最后一条所在行\n",
    "    history_size: 输入网络的时间跨度，即RNN循环层长度\n",
    "    target_size: 远期预测的时间跨度，若为下一期，则为0\n",
    "    single_step: 是否单步预测\n",
    "    '''\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, 1)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        #labels.append(dataset[i+target_size])\n",
    "        \n",
    "        if single_step:\n",
    "            labels.append(dataset[i+target_size])\n",
    "        else:\n",
    "            labels.append(dataset[i:i+target_size]) \n",
    "        \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "TRAIN_SPLIT = 144-12*3 #这里应该改为1220*0.8\n",
    "tf.random.set_seed(13)\n",
    "uni_data = df['AirPassengers']\n",
    "uni_data.index = df['Date']\n",
    "uni_data.head()\n",
    "uni_data = uni_data.values\n",
    "#print(uni_data[1])\n",
    "#print(len(uni_data))\n",
    "uni_train_mean = uni_data[:TRAIN_SPLIT].mean()\n",
    "uni_train_std = uni_data[:TRAIN_SPLIT].std()\n",
    "uni_data = (uni_data-uni_train_mean)/uni_train_std\n",
    "#print(uni_data[0:24])\n",
    "#print(uni_data[19])\n",
    "#print(uni_data[20])\n",
    "\n",
    "univariate_past_history = 12\n",
    "univariate_future_target = 12\n",
    "#若要单变量单步预测预测下一天，则为0,同时sigle_step=True\n",
    "#单变量远期预测，若要跳过本周，则为5，同时sigle_step=True\n",
    "#若要单变量多步预测预测下一天，则为0,同时sigle_step=False\n",
    "\n",
    "\n",
    "def create_time_steps(length):\n",
    "    return list(range(-length, 0))\n",
    "\n",
    "BATCH_SIZE = 6\n",
    "BUFFER_SIZE = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_uni, y_train_uni = univariate_data(uni_data, 0, TRAIN_SPLIT,\n",
    "                                           univariate_past_history,\n",
    "                                           univariate_future_target)\n",
    "x_val_uni, y_val_uni = univariate_data(uni_data, TRAIN_SPLIT, None,\n",
    "                                       univariate_past_history,\n",
    "                                       univariate_future_target)\n",
    "\n",
    "train_univariate = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))\n",
    "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_univariate = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni))\n",
    "val_univariate = val_univariate.batch(BATCH_SIZE).repeat()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集x:\n",
      "[[1.28090293]\n",
      " [1.02261341]\n",
      " [1.53919244]\n",
      " [1.37482639]\n",
      " [1.55093288]\n",
      " [2.39624402]\n",
      " [3.05370824]\n",
      " [3.21807429]\n",
      " [2.03229061]\n",
      " [1.50397115]\n",
      " [0.92868995]\n",
      " [1.24568163]\n",
      " [1.51571158]\n",
      " [1.30438379]\n",
      " [2.05577147]\n",
      " [1.93836715]\n",
      " [2.22013753]\n",
      " [2.83064002]\n",
      " [3.72291289]\n",
      " [3.85205765]\n",
      " [2.72497613]\n",
      " [2.06751191]\n",
      " [1.53919244]\n",
      " [2.04403104]]\n",
      "验证集y:\n",
      "2.184916231728903\n"
     ]
    }
   ],
   "source": [
    "print(\"验证集x:\")\n",
    "print(x_val_uni[0])\n",
    "print(\"验证集y:\")\n",
    "print(y_val_uni[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLSTMLayer = tf.keras.layers.RNN(\n",
    "    SLSTMCell(8,implementation=2,dropout=0.,use_bias=1), \n",
    "    input_shape = x_train_uni.shape[-2:], \n",
    "    dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build start\n",
      "build end\n"
     ]
    }
   ],
   "source": [
    "slstm_model = tf.keras.models.Sequential([\n",
    "    #tf.keras.layers.RNN(SLSTMCell(8,implementation=1,dropout=0.)),\n",
    "    SLSTMLayer,\n",
    "    #print(\"dense start\")\n",
    "    tf.keras.layers.Dense(1)\n",
    "    #print(\"dense end\")\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "slstm_model.compile(optimizer='adam', loss='mape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1)\n",
      "Train for 200 steps, validate for 12 steps\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 31s 154ms/step - loss: 69.7979 - val_loss: 50.4201\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 69.2265 - val_loss: 49.3211\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 27s 136ms/step - loss: 69.2753 - val_loss: 49.0367\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 26s 131ms/step - loss: 69.1686 - val_loss: 49.1167\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 28s 139ms/step - loss: 68.4605 - val_loss: 48.8711\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 34s 170ms/step - loss: 68.1363 - val_loss: 48.6718\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 28s 139ms/step - loss: 67.7699 - val_loss: 47.4589\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 39s 195ms/step - loss: 67.6383 - val_loss: 49.3110\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 51s 254ms/step - loss: 67.8021 - val_loss: 48.6637\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 50s 252ms/step - loss: 67.1531 - val_loss: 48.7076\n"
     ]
    }
   ],
   "source": [
    "for x, y in val_univariate.take(1):\n",
    "    print(slstm_model.predict(x).shape)\n",
    "\n",
    "EVALUATION_INTERVAL = 200 #这个值对训练结果的影响很大\n",
    "EPOCHS = 10\n",
    "\n",
    "# 注意：如不重新声明model，继续训练，则相当于在原有基础上继续训练；\n",
    "# 如要重新训练，则须重新声明\n",
    "slstm_history = slstm_model.fit(train_univariate, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=val_univariate, validation_steps=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n",
      "(6,)\n",
      "(12,)\n",
      "(12,)\n",
      "316.3952110789336\n",
      "391.0\n",
      "(12, 2)\n",
      "            0      1\n",
      "0  311.926378  417.0\n",
      "1  316.395211  391.0\n",
      "2  306.574195  419.0\n",
      "3  315.489188  461.0\n",
      "4  338.409919  472.0\n"
     ]
    }
   ],
   "source": [
    "validation_predict = np.array([])\n",
    "validation_real = np.array([])\n",
    "\n",
    "for x, y in val_univariate.take(int((36-univariate_past_history)/BATCH_SIZE)):\n",
    "    res = slstm_model.predict(x)\n",
    "    res = np.transpose(res)#转置\n",
    "    y = np.transpose(y)\n",
    "    print(y.shape)\n",
    "    validation_predict = np.concatenate([validation_predict, res[0]])#向量连接\n",
    "    validation_real = np.concatenate([validation_real, y])\n",
    "    \n",
    "print(validation_predict.shape) \n",
    "#之所以只有2批，因为测试集中刨去look_back=60, 1220-800-60=360，而批容量为256；\n",
    "#当批容量为25，则为360/\n",
    "#print(res)\n",
    "print(validation_real.shape)\n",
    "def recover_data(data, mean, std):\n",
    "    res = data * std + mean\n",
    "    return res\n",
    "validation_predict = recover_data(validation_predict, uni_train_mean, uni_train_std)\n",
    "validation_real = recover_data(validation_real, uni_train_mean, uni_train_std)\n",
    "print(validation_predict[1])\n",
    "print(validation_real[1])\n",
    "import pandas as pd\n",
    "#data = np.array([np.transpose(validation_predict), np.transpose(validation_real)])\n",
    "data = np.array([validation_predict, validation_real])\n",
    "data = np.transpose(data)\n",
    "print(data.shape)\n",
    "#slstm_validation_result = pd.DataFrame(data, columns=c(\"predict\",\"real\"))\n",
    "slstm_validation_result = pd.DataFrame(data)\n",
    "print(slstm_validation_result.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "slstm_validation_result.to_csv(\"AIR24_slstm_M2L24_2sd_tanh_single.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_history = univariate_past_history\n",
    "future_target = univariate_future_target\n",
    "dataset = uni_data\n",
    "x_train_multi, y_train_multi = univariate_data(dataset, 0,\n",
    "                                                TRAIN_SPLIT, past_history,\n",
    "                                                 future_target, single_step=True)\n",
    "x_val_multi, y_val_multi = univariate_data(dataset,\n",
    "                                             TRAIN_SPLIT, None, past_history,\n",
    "                                             future_target,single_step=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single window of past history : (12, 12, 1)\n",
      "\n",
      "Target temperature to predict : (12,)\n"
     ]
    }
   ],
   "source": [
    "print ('Single window of past history : {}'.format(x_val_multi.shape))\n",
    "print ('\\nTarget temperature to predict : {}'.format(y_val_multi.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_multi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_plot(history, true_future, prediction):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    num_in = create_time_steps(len(history))\n",
    "    num_out = len(true_future)\n",
    "\n",
    "    plt.plot(num_in, np.array(history), label='History')\n",
    "    plt.plot(np.arange(num_out), np.array(true_future), 'bo',\n",
    "           label='True Future')\n",
    "    if prediction.any():\n",
    "        plt.plot(np.arange(num_out), np.array(prediction), 'ro',\n",
    "             label='Predicted Future')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build start\n",
      "build end\n",
      "build start\n",
      "build end\n"
     ]
    }
   ],
   "source": [
    "# 多步预测使用两层LSTM，第一次为32维输出，第二层为16层输出\n",
    "# 模式2为c, 模式1为x或h\n",
    "multi_step_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.RNN(\n",
    "        SLSTMCell(32,implementation=1,dropout=0.,use_bias=1),\n",
    "        return_sequences=True,\n",
    "        input_shape = x_train_multi.shape[-2:], \n",
    "        dynamic=True),\n",
    "    tf.keras.layers.RNN(\n",
    "        SLSTMCell(16,implementation=1,dropout=0.,use_bias=1), #可对activation进行调整\n",
    "        dynamic=True),\n",
    "    tf.keras.layers.Dense(future_target)\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多步预测使用两层LSTM，第一次为32维输出，第二层为16层输出\n",
    "# 模式2为c, 模式1为x或h\n",
    "multi_step_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(32,\n",
    "        return_sequences=True,\n",
    "        input_shape = x_train_multi.shape[-2:]),\n",
    "    tf.keras.layers.LSTM(16),\n",
    "    tf.keras.layers.Dense(future_target)\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "multi_step_model = tf.keras.models.Sequential()\n",
    "multi_step_model.add(tf.keras.layers.LSTM(32,\n",
    "                                          return_sequences=True,\n",
    "                                          input_shape=x_train_multi.shape[-2:]))\n",
    "multi_step_model.add(tf.keras.layers.LSTM(16, activation='relu'))\n",
    "multi_step_model.add(tf.keras.layers.Dense(5))\n",
    "'''\n",
    "#multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), \n",
    "#                    loss='mae')\n",
    "multi_step_model.compile(optimizer='adam', loss='mape')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 12)\n",
      "(6, 12)\n"
     ]
    }
   ],
   "source": [
    "for x, y in val_data_multi.take(2):\n",
    "    print (multi_step_model.predict(x).shape)\n",
    "#查看预测结果的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10\n",
    "EVALUATION_INTERVAL = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 300 steps, validate for 60 steps\n",
      "Epoch 1/10\n",
      "300/300 [==============================] - 15s 49ms/step - loss: 96.2112 - val_loss: 54.8109\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 7s 23ms/step - loss: 71.7590 - val_loss: 56.9798\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 7s 24ms/step - loss: 67.8146 - val_loss: 44.8000\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 7s 23ms/step - loss: 66.2911 - val_loss: 54.2609\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 8s 28ms/step - loss: 63.7994 - val_loss: 54.7302\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 8s 27ms/step - loss: 61.6711 - val_loss: 50.6481\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 8s 27ms/step - loss: 60.3887 - val_loss: 58.2656\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 7s 24ms/step - loss: 58.4904 - val_loss: 52.4835\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 8s 27ms/step - loss: 58.0779 - val_loss: 51.2139\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 7s 23ms/step - loss: 55.7969 - val_loss: 55.9578\n"
     ]
    }
   ],
   "source": [
    "multi_step_history = multi_step_model.fit(train_data_multi, epochs=EPOCHS,\n",
    "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                          validation_data=val_data_multi,\n",
    "                                          validation_steps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_train_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-ff71b41370bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_train_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_step_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AirPassengers SLSTM Multi-Steps Training and validation loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_train_history' is not defined"
     ]
    }
   ],
   "source": [
    "plot_train_history(multi_step_history, 'AirPassengers SLSTM Multi-Steps Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 12)\n",
      "(6,)\n",
      "(6, 12)\n",
      "(6,)\n",
      "(12, 12)\n",
      "(2, 6)\n",
      "(12,)\n",
      "(6,)\n",
      "316.5201122245654\n",
      "391.0\n",
      "(2,)\n",
      "                                                   0\n",
      "0  [315.3316627455753, 316.5201122245654, 321.380...\n",
      "1         [417.0, 391.0, 419.0, 461.0, 472.0, 535.0]\n"
     ]
    }
   ],
   "source": [
    "# 数据导出\n",
    "# 多步预测的全部结果\n",
    "'''\n",
    "#之所以只取2个，因为只有2批\n",
    "#shape：因为测试集中刨去look_back=60, 1220-800-60-5=355 ，\n",
    "#为什么不是355+1=356？因为没有最后一条，也就是没有最后五天的预测，\n",
    "#但由于是多步预测，所以本质上只少了最后一天的预测\n",
    "#但为什么256-99会得到360？批次之间的序号问题导致的！应先合并再提取！\n",
    "#print(res)\n",
    "# 420-look_back-target = 类接力的维数\n",
    "'''\n",
    "validation_predictAll = np.array([])\n",
    "validation_realAll = np.array([])\n",
    "numVali = 1\n",
    "\n",
    "times = int((144-TRAIN_SPLIT-univariate_past_history-future_target)/BATCH_SIZE)\n",
    "for x, y in val_data_multi.take(times):\n",
    "    \n",
    "    res = multi_step_model(x)\n",
    "    #res = np.transpose(res)#转置\n",
    "    print(res.shape)\n",
    "    #y = np.transpose(y)\n",
    "    print(y.shape)\n",
    "    if numVali == 1:\n",
    "        validation_predictAll = res\n",
    "        validation_realAll = y\n",
    "    else:     \n",
    "        validation_predictAll = np.vstack((validation_predictAll, res))\n",
    "        validation_realAll = np.vstack((validation_realAll, y))\n",
    "    numVali = numVali + 1\n",
    "\n",
    "print(validation_predictAll.shape)\n",
    "print(validation_realAll.shape)\n",
    "\n",
    "# 多步预测的部分结果-转化为类接力集合\n",
    "validation_predict = np.array([])\n",
    "validation_real = np.array([])\n",
    "for i in range(validation_predictAll.shape[0]):\n",
    "    if i % future_target == 0:\n",
    "        validation_predict = np.concatenate([validation_predict, validation_predictAll[i]])#行向量连接\n",
    "        validation_real = np.concatenate([validation_real, validation_realAll[i]])\n",
    "\n",
    "print(validation_predict.shape) \n",
    "print(validation_real.shape)\n",
    "#应为testSize-lookBack-target=420-60-5=355, 缺最后1组target的预测值\n",
    "\n",
    "def recover_data(data, mean, std):\n",
    "    res = data * std + mean\n",
    "    return res\n",
    "\n",
    "validation_predict = recover_data(validation_predict, uni_train_mean, uni_train_std)\n",
    "validation_real = recover_data(validation_real, uni_train_mean, uni_train_std)\n",
    "print(validation_predict[1])\n",
    "print(validation_real[1])\n",
    "\n",
    "import pandas as pd\n",
    "#data = np.array([np.transpose(validation_predict), np.transpose(validation_real)])\n",
    "data = np.array([validation_predict, validation_real])\n",
    "data = np.transpose(data)\n",
    "print(data.shape)\n",
    "lstm_validation_result = pd.DataFrame(data)\n",
    "print(lstm_validation_result.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_validation_result.to_csv(\"AIR_lstm_multi.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
