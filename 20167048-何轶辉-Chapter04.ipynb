{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.distribute import distribution_strategy_context as ds_context\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.keras import activations\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras import constraints\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.engine.base_layer import Layer\n",
    "from tensorflow.python.keras.engine.input_spec import InputSpec\n",
    "from tensorflow.python.keras.utils import generic_utils\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import control_flow_util\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.training.tracking import base as trackable\n",
    "from tensorflow.python.training.tracking import data_structures\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "from tensorflow.tools.docs import doc_controls\n",
    "\n",
    "class SLSTMCell(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    继承Layer并重载了5个函数，参照TF中LSTMCell源码并做了修改，源码见第 6.3.1.3 节（上一节）\n",
    "    随机遮挡来自类DropoutRNNCellMixin\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                   units,\n",
    "                   activation='tanh',\n",
    "                   recurrent_activation='hard_sigmoid',\n",
    "                   use_bias=True,\n",
    "                   kernel_initializer='glorot_uniform',\n",
    "                   recurrent_initializer='orthogonal',\n",
    "                   bias_initializer='zeros',\n",
    "                   unit_forget_bias=True,\n",
    "                   kernel_regularizer=None,\n",
    "                   recurrent_regularizer=None,\n",
    "                   bias_regularizer=None,\n",
    "                   kernel_constraint=None,\n",
    "                   recurrent_constraint=None,\n",
    "                   bias_constraint=None,\n",
    "                   dropout=0.,\n",
    "                   recurrent_dropout=0.,\n",
    "                   implementation=1,\n",
    "                   **kwargs):\n",
    "        #print(\"init start\")\n",
    "        self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n",
    "        super(SLSTMCell, self).__init__(**kwargs)\n",
    "        self.num_gates = 6\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.unit_forget_bias = unit_forget_bias\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        self.dropout = min(1., max(0., dropout))\n",
    "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "        if self.recurrent_dropout != 0 and implementation != 1:\n",
    "            logging.debug(RECURRENT_DROPOUT_WARNING_MSG)\n",
    "            self.implementation = 1\n",
    "        else:\n",
    "            self.implementation = implementation\n",
    "        # tuple(_ListWrapper) was silently dropping list content in at least 2.7.10,\n",
    "        # and fixed after 2.7.16. Converting the state_size to wrapper around\n",
    "        # NoDependency(), so that the base_layer.__setattr__ will not convert it to\n",
    "        # ListWrapper. Down the stream, self.states will be a list since it is\n",
    "        # generated from nest.map_structure with list, and tuple(list) will work\n",
    "        # properly.\n",
    "        self.state_size = data_structures.NoDependency([self.units, self.units])\n",
    "        self.output_size = self.units\n",
    "        \n",
    "        self.SeasonalGate = False\n",
    "        \n",
    "        #print(\"init end\")\n",
    "\n",
    "    \n",
    "    \n",
    "    @tf_utils.shape_type_conversion\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print(\"build start\")\n",
    "\n",
    "        input_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\n",
    "            # W_x: 用于和当前时刻隐层输入x做乘法的矩阵\n",
    "            # 加入季节门，将self.units * 4改为self.units * 6 \n",
    "            # 这里\n",
    "            shape=(input_dim, self.units * self.num_gates),\n",
    "            name='kernel',\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint)\n",
    "\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            # W_h: 用于和前一时刻隐层输出h做乘法的矩阵\n",
    "            # 加入季节门，将self.units * 4改为self.units * 6\n",
    "            shape=(self.units, self.units * self.num_gates),\n",
    "            name='recurrent_kernel',\n",
    "            initializer=self.recurrent_initializer,\n",
    "            regularizer=self.recurrent_regularizer,\n",
    "            constraint=self.recurrent_constraint)\n",
    "\n",
    "\n",
    "        if self.use_bias:\n",
    "            if self.unit_forget_bias:\n",
    "\n",
    "                def bias_initializer(_, *args, **kwargs):\n",
    "                    return K.concatenate([\n",
    "                          self.bias_initializer((self.units,), *args, **kwargs),\n",
    "                          initializers.Ones()((self.units,), *args, **kwargs),\n",
    "                          self.bias_initializer((self.units * (self.num_gates - 2),), *args, **kwargs),\n",
    "                      ])\n",
    "            else:\n",
    "                bias_initializer = self.bias_initializer\n",
    "            \n",
    "            self.bias = self.add_weight(\n",
    "                  shape=(self.units * self.num_gates,),\n",
    "                  name='bias',\n",
    "                  initializer=bias_initializer,\n",
    "                  regularizer=self.bias_regularizer,\n",
    "                  constraint=self.bias_constraint)\n",
    "\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.built = True\n",
    "        print(\"build end\")\n",
    "\n",
    "\n",
    "    def _compute_carry_and_output(self, x, h_tm1, c_tm1):\n",
    "        \"\"\"Computes carry and output using split kernels.\"\"\"\n",
    "        '''\n",
    "        模式1\n",
    "        采用分离法计算各个门控单元的值\n",
    "        本模式尝试将选通器的判断方法调整为h_tm1和x\n",
    "        本方法中x,h_tm1,c_tm1均为各个门加mask后拼成的元组，故需要多传多\n",
    "        模式1中的x是 Wx+b \n",
    "        '''\n",
    "        x_i, x_f, x_c, x_o, x_s1, x_s2 = x\n",
    "        h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o, h_tm1_s1, h_tm1_s2 = h_tm1\n",
    "        i = self.recurrent_activation(\n",
    "            x_i + K.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]))\n",
    "        f = self.recurrent_activation(x_f + K.dot(\n",
    "            h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))\n",
    "        \n",
    "        # 季节门\n",
    "        \n",
    "        sdB2 = 1.82\n",
    "        sd = 2.41\n",
    "        sdT2 = 3.58\n",
    "\n",
    "        # 季节门1【改】\n",
    "        s1 = K.tanh(x_s1 + K.dot(\n",
    "            h_tm1_s1, self.recurrent_kernel[:, self.units * 4:self.units * 5]))\n",
    "        # 季节门2【改】\n",
    "        s2 = self.recurrent_activation(x_s2 + K.dot(\n",
    "            h_tm1_s2, self.recurrent_kernel[:, self.units * 5:]))\n",
    "        #【改】\n",
    "        #SeasonalGate = K.max(x_s1) - K.min(x_s1) > 1.82\n",
    "        if self.SeasonalGate:\n",
    "            \n",
    "            c = f * c_tm1 * s1 + i * self.activation(x_c + K.dot(\n",
    "                h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))\n",
    "            o = self.recurrent_activation(\n",
    "                x_o + K.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:self.units * 4]))\n",
    "        else:\n",
    "            c = f * c_tm1 * s2 + i * self.activation(x_c + K.dot(\n",
    "                h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))\n",
    "            o = self.recurrent_activation(\n",
    "                x_o + K.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:self.units * 4]))\n",
    "\n",
    "        return c, o\n",
    "\n",
    "    def _compute_carry_and_output_fused(self, z, c_tm1):\n",
    "        \"\"\"\n",
    "        模式2\n",
    "        Computes carry and output using fused kernels.\n",
    "        使用合并向量的计算方法\n",
    "        \"\"\"\n",
    "        \n",
    "        z0, z1, z2, z3, z4, z5 = z\n",
    "        i = self.recurrent_activation(z0)\n",
    "        f = self.recurrent_activation(z1)\n",
    "        \n",
    "        #季节门\n",
    "        sdB4 = 0.2828\n",
    "        sdB2 = 0.5102\n",
    "        sd = 0.7975\n",
    "        sdT2 = 0.9501\n",
    "        #if True:\n",
    "        if K.max(K.softmax(c_tm1)) - K.min(K.softmax(c_tm1)) > 0.7975: #【改】\n",
    "            #季节门1【改】\n",
    "            s = K.tanh(z4)\n",
    "            #s = K.square(s)\n",
    "        else:\n",
    "            #季节门2【改】\n",
    "            s = self.recurrent_activation(z5)\n",
    "            \n",
    "        #c = f * c_tm1 + i * self.activation(z2)\n",
    "        c = f * c_tm1 * s + i * self.activation(z2)\n",
    "        o = self.recurrent_activation(z3)\n",
    "        return c, o\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        num_gates = 6\n",
    "        h_tm1 = states[0]  # previous memory state\n",
    "        c_tm1 = states[1]  # previous carry state\n",
    "\n",
    "        #dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=num_gates)\n",
    "        \n",
    "        #rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n",
    "        #    h_tm1, training, count=num_gates)\n",
    "\n",
    "        if self.implementation == 1:\n",
    "            '''\n",
    "            implementation是计算模式，1或者2\n",
    "            '''\n",
    "            if  K.mean(inputs) > 1 : #即inputs值大于1倍标准差\n",
    "                self.SeasonalGate = True\n",
    "\n",
    "            if 0 < self.dropout < 1.:\n",
    "                inputs_i = inputs * dp_mask[0]\n",
    "                inputs_f = inputs * dp_mask[1]\n",
    "                inputs_c = inputs * dp_mask[2]\n",
    "                inputs_o = inputs * dp_mask[3]\n",
    "                inputs_s1 = inputs * dp_mask[4]\n",
    "                inputs_s2 = inputs * dp_mask[5]\n",
    "            else:\n",
    "                inputs_i = inputs\n",
    "                inputs_f = inputs\n",
    "                inputs_c = inputs\n",
    "                inputs_o = inputs\n",
    "                inputs_s1 = inputs\n",
    "                inputs_s2 = inputs\n",
    "            k_i, k_f, k_c, k_o, k_s1, k_s2 = array_ops.split(\n",
    "                  self.kernel, num_or_size_splits=6, axis=1)\n",
    "            x_i = K.dot(inputs_i, k_i)\n",
    "            x_f = K.dot(inputs_f, k_f)\n",
    "            x_c = K.dot(inputs_c, k_c)\n",
    "            x_o = K.dot(inputs_o, k_o)\n",
    "            x_s1 = K.dot(inputs_o, k_s1)\n",
    "            x_s2 = K.dot(inputs_o, k_s2)\n",
    "            if self.use_bias:\n",
    "                b_i, b_f, b_c, b_o, b_s1, b_s2 = array_ops.split(\n",
    "                    self.bias, num_or_size_splits=6, axis=0)\n",
    "                x_i = K.bias_add(x_i, b_i)\n",
    "                x_f = K.bias_add(x_f, b_f)\n",
    "                x_c = K.bias_add(x_c, b_c)\n",
    "                x_o = K.bias_add(x_o, b_o)\n",
    "                x_s1 = K.bias_add(x_o, b_s1)\n",
    "                x_s2 = K.bias_add(x_o, b_s2)\n",
    "\n",
    "            if 0 < self.recurrent_dropout < 1.:\n",
    "                h_tm1_i = h_tm1 * rec_dp_mask[0]\n",
    "                h_tm1_f = h_tm1 * rec_dp_mask[1]\n",
    "                h_tm1_c = h_tm1 * rec_dp_mask[2]\n",
    "                h_tm1_o = h_tm1 * rec_dp_mask[3]\n",
    "                h_tm1_s1 = h_tm1 * rec_dp_mask[4]\n",
    "                h_tm1_s2 = h_tm1 * rec_dp_mask[5]\n",
    "            else:\n",
    "                h_tm1_i = h_tm1\n",
    "                h_tm1_f = h_tm1\n",
    "                h_tm1_c = h_tm1\n",
    "                h_tm1_o = h_tm1\n",
    "                h_tm1_s1 = h_tm1\n",
    "                h_tm1_s2 = h_tm1\n",
    "            x = (x_i, x_f, x_c, x_o, x_s1, x_s2)\n",
    "            h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o,h_tm1_s1,h_tm1_s2)\n",
    "            c, o = self._compute_carry_and_output(x, h_tm1, c_tm1)\n",
    "        else:\n",
    "            # 模式2\n",
    "            # 以下为使用随机遮挡的方法\n",
    "            if 0. < self.dropout < 1.:\n",
    "                inputs = inputs * dp_mask[0]\n",
    "            \n",
    "            z = K.dot(inputs, self.kernel)\n",
    "            #不是z = K.dot(self.kernel, inputs)，因为x_t是行向量\n",
    "            z += K.dot(h_tm1,self.recurrent_kernel)\n",
    "            #不是z += K.dot(self.recurrent_kernel, h_tm1)，因为x_t是行向量\n",
    "            if self.use_bias:\n",
    "                z = K.bias_add(z, self.bias)\n",
    "            \n",
    "            z = array_ops.split(z, num_or_size_splits=num_gates, axis=1)\n",
    "            c, o = self._compute_carry_and_output_fused(z, c_tm1)\n",
    "\n",
    "        h = o * self.activation(c)\n",
    "        return h, [h, c]\n",
    "    \n",
    "    def get_config(self):\n",
    "        print(\"config start\")\n",
    "\n",
    "        config = {\n",
    "            'units':\n",
    "                self.units,\n",
    "            'activation':\n",
    "                activations.serialize(self.activation),\n",
    "            'recurrent_activation':\n",
    "                activations.serialize(self.recurrent_activation),\n",
    "            'use_bias':\n",
    "                self.use_bias,\n",
    "            'kernel_initializer':\n",
    "                initializers.serialize(self.kernel_initializer),\n",
    "            'recurrent_initializer':\n",
    "                initializers.serialize(self.recurrent_initializer),\n",
    "            'bias_initializer':\n",
    "                initializers.serialize(self.bias_initializer),\n",
    "            'unit_forget_bias':\n",
    "                self.unit_forget_bias,\n",
    "            'kernel_regularizer':\n",
    "                regularizers.serialize(self.kernel_regularizer),\n",
    "            'recurrent_regularizer':\n",
    "                regularizers.serialize(self.recurrent_regularizer),\n",
    "            'bias_regularizer':\n",
    "                regularizers.serialize(self.bias_regularizer),\n",
    "            'kernel_constraint':\n",
    "                constraints.serialize(self.kernel_constraint),\n",
    "            'recurrent_constraint':\n",
    "                constraints.serialize(self.recurrent_constraint),\n",
    "            'bias_constraint':\n",
    "                constraints.serialize(self.bias_constraint),\n",
    "            'dropout':\n",
    "                self.dropout,\n",
    "            'recurrent_dropout':\n",
    "                self.recurrent_dropout,\n",
    "            'implementation':\n",
    "                self.implementation\n",
    "        }\n",
    "        base_config = super(SLSTMCell, self).get_config()\n",
    "        print(\"config end\")\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "        \n",
    "\n",
    "    '''\n",
    "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "        return list(_generate_zero_filled_state_for_cell(\n",
    "            self, inputs, batch_size, dtype))\n",
    "    \n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "### 导入 TensorFlow 和其他库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "修改下面一行代码，在你自己的数据上运行此代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_file = 'economist.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'EducationContext.txt'\n",
    "path_to_fileAll = 'kaoyanyingyu.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "### 读取数据\n",
    "\n",
    "首先，看一看文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aavnuByVymwK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 2881 characters\n"
     ]
    }
   ],
   "source": [
    "# 读取并为 py2 compat 解码\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "textAll = open(path_to_fileAll, 'rb').read().decode(encoding='gbk')\n",
    "\n",
    "# 文本长度是指文本中的字符个数\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Duhg9NrUymwO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relationship between formal education and economic growth in poor countries is widely misunderstood by economists and politicians alike progress in both area is undoubtedly necessary for the social, political and intellectual development of these\n"
     ]
    }
   ],
   "source": [
    "# 看一看文本中的前 250 个字符\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlCgQBRVymwR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 unique characters\n"
     ]
    }
   ],
   "source": [
    "# 文本中的非重复字符\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-coding without Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量化单词（不包含标点）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该篇文章的词汇量：256\n",
      "考研词汇量：14176\n"
     ]
    }
   ],
   "source": [
    "# Word-coding\n",
    "# 词典集合（不重复）--dt\n",
    "tz = tfds.features.text.Tokenizer() \n",
    "tzAll = tfds.features.text.Tokenizer() #全文词典\n",
    "\n",
    "dt = set() #创建集合，可以忽略重复部分\n",
    "dtAll =set() #创建所有词汇的词典\n",
    "\n",
    "#textNew = np.array(['cripple read go'])\n",
    "#textNew = u'cripple read go' #unicode string\n",
    "\n",
    "# 取出该篇文章中所有词汇\n",
    "tzs = tz.tokenize(text)\n",
    "dt.update(tzs) #其实也可以创建字典-枚举类型\n",
    "# 取出所有考研文章中的词汇\n",
    "tzsAll = tzAll.tokenize(textAll)\n",
    "dtAll.update(tzsAll)\n",
    "\n",
    "dtSize = len(dt)\n",
    "print(\"该篇文章的词汇量：\"+str(dtSize))\n",
    "dtSizeAll = len(dtAll)\n",
    "print(\"考研词汇量：\"+str(dtSizeAll))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该篇文章的单次数量：445\n"
     ]
    }
   ],
   "source": [
    "# 词集（有重复）--textWord--将该篇文章拆分为单词数组\n",
    "textWord = np.array(tzs)\n",
    "textWordSize = len(textWord)\n",
    "print(\"该篇文章的单次数量：\"+str(textWordSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造词典--使用全考研词汇集合dtAll\n",
    "# 词典 <- 枚举 <- 词典集合\n",
    "dtAll = sorted(dtAll) #正序化\n",
    "word2idx = {u:i for i, u in enumerate(dtAll)}\n",
    "idx2word = np.array(dtAll)\n",
    "\n",
    "#text_as_word = np.array([word2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '0' :   0,\n",
      "  '00':   1,\n",
      "  '000':   2,\n",
      "  '00pm':   3,\n",
      "  '075':   4,\n",
      "  '1' :   5,\n",
      "  '10':   6,\n",
      "  '100':   7,\n",
      "  '105':   8,\n",
      "  '108':   9,\n",
      "  '10Part':  10,\n",
      "  '10th':  11,\n",
      "  '11':  12,\n",
      "  '110':  13,\n",
      "  '1177':  14,\n",
      "  '119':  15,\n",
      "  '12':  16,\n",
      "  '120':  17,\n",
      "  '1200s':  18,\n",
      "  '121':  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for word,_ in zip(word2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(word), word2idx[word]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide compelling PhD 1909\n"
     ]
    }
   ],
   "source": [
    "a = [2893,4777,2100,77]\n",
    "print(' '.join(idx2word[a])) #使用字符串连接函数join()，其调用者为分隔符\n",
    "#print(word2idx[','])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测任务\n",
    "给定一个单词或者一个单词序列，下一个最可能出现的单词是什么？这就是我们训练模型要执行的任务。输入进模型的是一个单词序列，我们训练这个模型来预测输出 -- 每个时间步（time step）预测下一个单词是什么。\n",
    "\n",
    "RNN 是根据前面看到的元素维持内部状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建训练样本和目标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全文编码\n",
    "textAsWord = np.array([word2idx[c] for c in textWord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(['The', 'relationship', 'between', 'formal', 'education', 'and',\n",
      "       'economic', 'growth', 'in', 'poor', 'countries', 'is', 'widely'],\n",
      "      dtype='<U13') ---- words mapped to int ---- > [ 2658 10832  4019  6908  6045  3513  6024  7268  7765 10024  5162  8197\n",
      " 13488]\n"
     ]
    }
   ],
   "source": [
    "# 显示文本首 13 个单词的整数映射\n",
    "print ('{} ---- words mapped to int ---- > {}'.format(repr(textWord[:13]), textAsWord[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "relationship\n",
      "between\n"
     ]
    }
   ],
   "source": [
    "# 设定每个输入句子长度的最大值, 这是look_back\n",
    "seq_length = 1 #sequence_length \n",
    "# textWordSize是文集的长度\n",
    "# examples_per_epoch = textWordSize//seq_length # 舍去余数\n",
    "\n",
    "# 创建训练样本 / 目标\n",
    "wordDataset = tf.data.Dataset.from_tensor_slices(textAsWord)\n",
    "\n",
    "for i in wordDataset.take(3):\n",
    "    print(idx2word[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The relationship'\n",
      "'between formal'\n",
      "'education and'\n"
     ]
    }
   ],
   "source": [
    "#batch的作用：在训练集中创建特定长度的序列，即批（batch）\n",
    "# 在这里批容量为sequence_length+1\n",
    "sequences = wordDataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(3):\n",
    "    print(repr(' '.join(idx2word[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The relationship'\n",
      "'between formal'\n",
      "'education and'\n"
     ]
    }
   ],
   "source": [
    "for item in sequences.take(3): #任意3条，每一条6个单词\n",
    "    print(repr(' '.join(idx2word[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1] #左闭右开，最后一个元素不取\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text #这是返回的元组吗？\n",
    "\n",
    "dataset = sequences.map(split_input_target) #包含（训练集内容，训练集标签）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'The'\n",
      "Target data: 'relationship'\n"
     ]
    }
   ],
   "source": [
    "# 查看一组内容-标签，分别为上一个词和下一个词\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(' '.join(idx2word[input_example.numpy()])))\n",
    "    print ('Target data:', repr(' '.join(idx2word[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建批次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((10, 1), (10, 1)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "# 批大小 ; 这里的批大小与文本长度和序列长度有关，本实验文本长度2290，序列长度设为100\n",
    "BATCH_SIZE = 10 \n",
    "\n",
    "# 设定缓冲区大小，以重新排列数据集\n",
    "# （TF 数据被设计为可以处理可能是无限的序列，\n",
    "# 所以它不会试图在内存中重新排列整个序列。相反，\n",
    "# 它维持一个缓冲区，在缓冲区重新排列元素。） \n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建词典模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = [\"e\",\"a\"]\n",
    "b = \"e\"\n",
    "j = b in a\n",
    "c = False\n",
    "print(c&j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "# 词典集的长度（不重复）\n",
    "print(dtSize)\n",
    "\n",
    "# 嵌入的维度\n",
    "embedding_dim = 256 #特征空间的维度\n",
    "\n",
    "# RNN 的单元数量\n",
    "#rnn_units = 1024\n",
    "#run_units = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.RNN(\n",
    "            SLSTMCell(32,implementation=1,dropout=0.,use_bias=1,recurrent_initializer='glorot_uniform'),\n",
    "            return_sequences=True,\n",
    "            stateful=True,\n",
    "            dynamic=True)),\n",
    "        tf.keras.layers.RNN(\n",
    "            SLSTMCell(16,implementation=1,dropout=0.,use_bias=1,recurrent_initializer='glorot_uniform'),\n",
    "            return_sequences=True,\n",
    "            stateful=True,\n",
    "            dynamic=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对照\n",
    "import tensorflow as tf\n",
    "def build_model_duizhao(vocab_size, embedding_dim, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,\n",
    "                             return_sequences=True,\n",
    "                             stateful=True,\n",
    "                             recurrent_initializer='glorot_uniform')),\n",
    "        tf.keras.layers.LSTM(16,\n",
    "                             return_sequences=True,\n",
    "                             stateful=True,\n",
    "                             recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config start\n",
      "config end\n",
      "config start\n",
      "config end\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = dtSizeAll,\n",
    "  embedding_dim=embedding_dim,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对照模型：LSTM\n",
    "model = build_model_duizhao(\n",
    "  vocab_size = dtSizeAll,\n",
    "  embedding_dim=embedding_dim,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 试试词典模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build start\n",
      "build end\n",
      "build start\n",
      "build end\n",
      "build start\n",
      "build end\n",
      "(10, 1, 14176) # (batch_size, sequence_length, vocab_size)\n",
      "tf.Tensor(\n",
      "[[ 3100]\n",
      " [ 7459]\n",
      " [10681]\n",
      " [10558]\n",
      " [11741]\n",
      " [12519]\n",
      " [ 3202]\n",
      " [ 9801]\n",
      " [ 9539]\n",
      " [ 6045]], shape=(10, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 检查输出形状\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    \n",
    "print(input_example_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'a'\n",
      "\n",
      "Next Word Predictions: \n",
      " 'host'\n"
     ]
    }
   ],
   "source": [
    "# example_batch_predictions是未经训练的模型的预测值\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "print(\"Input: \\n\", repr(\" \".join(idx2word[(input_example_batch[0]).numpy()])))\n",
    "print()\n",
    "print(\"Next Word Predictions: \\n\", repr(\" \".join(idx2word[sampled_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (10, None, 256)           3629056   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (10, None, 64)            110976    \n",
      "_________________________________________________________________\n",
      "rnn_7 (RNN)                  (10, None, 16)            7776      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (10, None, 14176)         240992    \n",
      "=================================================================\n",
      "Total params: 3,988,800\n",
      "Trainable params: 3,988,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练词典模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (10, 1, 14176)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       9.559317\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "    #return tf.keras.losses.categorical_crossentropy(labels, logits, from_logits=True)\n",
    "    #return tf.keras.losses.cosine_proximity(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查点保存至的目录\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "# 检查点的文件名\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "22/22 [==============================] - 4s 161ms/step - loss: 9.5547\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 4s 167ms/step - loss: 9.5208\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 4s 160ms/step - loss: 9.3229\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 4s 163ms/step - loss: 8.1240\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 4s 169ms/step - loss: 7.2527\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 4s 186ms/step - loss: 6.6254\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 4s 168ms/step - loss: 6.1504\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 4s 173ms/step - loss: 5.7877\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 4s 192ms/step - loss: 5.5434\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 4s 181ms/step - loss: 5.3517\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 4s 178ms/step - loss: 5.2415\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 4s 165ms/step - loss: 5.1307\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 3s 156ms/step - loss: 5.0676\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 4s 161ms/step - loss: 5.0198\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 4s 166ms/step - loss: 4.9827\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 3s 157ms/step - loss: 4.9626\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 4s 175ms/step - loss: 4.9277\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 4s 174ms/step - loss: 4.9183\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 4s 162ms/step - loss: 4.8948\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 3s 158ms/step - loss: 4.9009\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 4s 168ms/step - loss: 4.8881\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 4s 170ms/step - loss: 4.8746\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 4s 158ms/step - loss: 4.8562\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 4s 161ms/step - loss: 4.8537\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 4s 162ms/step - loss: 4.8551\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 4s 171ms/step - loss: 4.8429\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 4s 161ms/step - loss: 4.8452\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 4s 161ms/step - loss: 4.8293\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 4s 184ms/step - loss: 4.8335\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 4s 182ms/step - loss: 4.8210\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "#history = model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "def plot_train_history(history, title):\n",
    "    loss = history.history['loss']\n",
    "    #val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(loss))\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    #plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train_history(history,\"Training loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 备份多输入输出模型 pastoral pickle intricacy award\n",
    "model_copy = model\n",
    "#model = model_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config start\n",
      "config end\n",
      "config start\n",
      "config end\n"
     ]
    }
   ],
   "source": [
    "# 重新生成一个单输出模型\n",
    "# 调整为单输入输出，即batch_size=1\n",
    "model = build_model(dtSizeAll, embedding_dim, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意这是对照\n",
    "# 调整为单输入输出，即batch_size=1\n",
    "model = build_model_duizhao(dtSizeAll, embedding_dim, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练权重，在TF2.5.0中这个函数的写法是否有变化，查看相应API\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) # 返回最新的训练权重\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多变量多步接力预测\n",
    "def generate_textWord(model, start_string,num_generate):\n",
    "    # 评估步骤（用学习过的模型生成文本）\n",
    "\n",
    "    # 要生成的单词个数\n",
    "    #num_generate = num_generate\n",
    "    \n",
    "    # 将start_string拆分\n",
    "    tz = tfds.features.text.Tokenizer()\n",
    "    #dt = set() #创建集合，可以忽略重复部分\n",
    "    tzs = tz.tokenize(start_string)\n",
    "    #start_string = tzs\n",
    "    #start_string = tzs.numpy()\n",
    "\n",
    "    # 将起始字符串转换为数字（向量化）\n",
    "    input_eval = [word2idx[s] for s in tzs]\n",
    "    input_eval = tf.expand_dims(input_eval, 0) #新加第0维度，为了能代入模型，模型中第0维是批量\n",
    "\n",
    "    # 空列表用于存储结果\n",
    "    text_generated = []\n",
    "\n",
    "    # 低温度会生成更可预测的文本 #?\n",
    "    # 较高温度会生成更令人惊讶的文本\n",
    "    # 可以通过试验以找到最好的设定\n",
    "    temperature = 0.8\n",
    "\n",
    "    # 这里批大小为 1\n",
    "    #model.reset_states() # 对于自定义的RNN，在TF2.5.0中不能包含此句\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # 删除批次的维度\n",
    "        predictions = tf.squeeze(predictions, 0) #删去第0维度\n",
    "\n",
    "        # 用分类分布预测模型返回的字符\n",
    "        predictions = predictions / temperature #为什么？\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        # tf.random.categorical 把输入当做权重得分，维度为batch_size x num_classes，\n",
    "        # 其实可以理解为softmax过程\n",
    "        # 在这里num_classes就是词表长度\n",
    "        # num_samples即是抽样次数，超参数\n",
    "        # 返回值为抽样结果（对每个batch进行多次抽样）维度为：batch_size x num_samples\n",
    "        # 相当于在softmax的基础上增加了随机扰动\n",
    "        \n",
    "        # 把预测字符和前面的隐藏状态一起传递给模型作为下一个输入\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2word[predicted_id]) #predicted_id是位置坐标\n",
    "        \n",
    "        res = ' '.join(text_generated) #添加一个新词\n",
    "\n",
    "    return(start_string+\" \"+res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.categorical([[-1.20397282,-0.9162907,-1.60943794,-2.30258512],[-1.20397282,-0.9162907,-1.60943794,-2.30258512]],num_samples=6)[-1,0].numpy()\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"The author holds in Paragraph 1 that the importance of education in poor countries\"\n",
    "answer1 = \"has been overestimated\"\n",
    "strstr = [question, answer1]\n",
    "num_strstr = len(strstr[1].split())\n",
    "num_strstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acchyh(num_samples):\n",
    "    num_res = 0\n",
    "    for i in range(num_samples):\n",
    "        res = generate_textWord(model,\"the relationship between education\",10)\n",
    "        res = res.split()\n",
    "        keywords = [\"economic\",\"economy\",\"economist\"]\n",
    "        jugement = False\n",
    "        for j in range(len(keywords)):\n",
    "            jugement = jugement | (keywords[j] in res)\n",
    "        if jugement:\n",
    "            num_res = num_res +1\n",
    "    return(num_res/num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acchyh(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 备份单输入输出模型\n",
    "#slstm_sigmoid_1by2 = model\n",
    "#lstm_sigmoid = model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
